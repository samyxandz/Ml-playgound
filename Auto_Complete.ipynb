{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+03V4iN8XvYcyaqURm6Iv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyxandz/Ml-playgound/blob/main/Auto_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Models: Auto-Complete\n"
      ],
      "metadata": {
        "id": "a6Hay2_uBS7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Prototype uses N-grams, a simple but powerful method for language modeling."
      ],
      "metadata": {
        "id": "rnr3BRqbBXpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RoadMap\n",
        "\n",
        "<br>\n",
        "\n",
        "1.   Load and preprocess data\n",
        "Load and tokenize data.\n",
        "\n",
        "*   Load and tokenize data.\n",
        "*   Split the sentences into train and test sets.\n",
        "*   Replace words with a low frequency by an unknown marker <unk>.\n",
        "\n",
        "\n",
        "2.   Develop N-gram based language models\n",
        "\n",
        "\n",
        "*   Compute the count of n-grams from a given data set.\n",
        "*   Estimate the conditional probability of a next word with k-smoothing.\n",
        "\n",
        "3. Evaluate the N-gram models by computing the perplexity score.\n",
        "\n",
        "4. Use your own model to suggest an upcoming word given your sentence.\n",
        "\n"
      ],
      "metadata": {
        "id": "XI3oaszzBqB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.data.path.append('.')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRmB5nvlHCiY",
        "outputId": "6a0a99a2-c06f-410c-bd72-ca3e69bf57e6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load the data\n"
      ],
      "metadata": {
        "id": "UeqnsFkVHhJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"en_US.twitter.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "display(data[0:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "em2ODgXaHjXd",
        "outputId": "407bff06-7d5c-4e3f-93a9-5baa3bdc6732"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-process the data\n",
        "\n",
        "reprocess this data with the following steps:\n",
        "\n",
        "1. Split data into sentences using \"\\n\" as the delimiter.\n",
        "2. Split each sentence into tokens.\n",
        "3. Assign sentences into train or test sets.\n",
        "4. Find tokens that appear at least N times in the training data.\n",
        "5. Replace tokens that appear less than N times by <unk>"
      ],
      "metadata": {
        "id": "8EfSAVNUIFYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into sentences\n",
        "\n",
        "    Split data by linebreak \"\\n\"\n",
        "    Args:\n",
        "        data: str\n",
        "    Returns:\n",
        "        A list of sentences\n"
      ],
      "metadata": {
        "id": "3wQIRcwtIoDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_to_sentences(data):\n",
        "\n",
        "    sentences = data.split('\\n')\n",
        "\n",
        "    # - Remove leading and trailing spaces from each sentence\n",
        "    # - Drop sentences if they are empty strings.\n",
        "    sentences = [sentence.strip() for sentence in sentences]\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "H79n1nryIR0I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test your code\n",
        "x = \"\"\"I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\"\"\"\n",
        "print(x)\n",
        "\n",
        "split_to_sentences(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3woV0lfiLWL5",
        "outputId": "dbfd08c4-5bf5-4d08-94aa-81684b905025"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have a pen.\n",
            "I have an apple. \n",
            "Ah\n",
            "Apple pen.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize sentences (split a sentence into a list of words).\n",
        "\n",
        "* Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words.\n",
        "* Append each tokenized list of words into a list of tokenized sentences\n",
        "\n",
        "\n",
        "    Tokenize sentences into tokens (words)\n",
        "    \n",
        "    Args:\n",
        "        sentences: List of strings\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of tokens"
      ],
      "metadata": {
        "id": "K8EBvfbILUIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences(sentences):\n",
        "\n",
        "    tokenized_sentences = []\n",
        "\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "        # Convert to lowercase letters\n",
        "        sentence = sentence.lower()\n",
        "\n",
        "        # Convert into a list of words\n",
        "        tokenized = nltk.word_tokenize(sentence)\n",
        "\n",
        "        # append the list of words to the list of lists\n",
        "        tokenized_sentences.append(tokenized)\n",
        "\n",
        "    return tokenized_sentences\n"
      ],
      "metadata": {
        "id": "vx95qECkMU9G"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
        "tokenize_sentences(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6exUuJ6Mv-f",
        "outputId": "379dfc20-bb6c-4ef1-9f67-b3199f0f6542"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['sky', 'is', 'blue', '.'],\n",
              " ['leaves', 'are', 'green', '.'],\n",
              " ['roses', 'are', 'red', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}