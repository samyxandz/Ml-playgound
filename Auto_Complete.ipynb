{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpb78a1h+F5mMBzQaC52x/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyxandz/Ml-playgound/blob/main/Auto_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Models: Auto-Complete\n"
      ],
      "metadata": {
        "id": "a6Hay2_uBS7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Prototype uses N-grams, a simple but powerful method for language modeling."
      ],
      "metadata": {
        "id": "rnr3BRqbBXpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RoadMap\n",
        "\n",
        "<br>\n",
        "\n",
        "1.   Load and preprocess data\n",
        "Load and tokenize data.\n",
        "\n",
        "*   Load and tokenize data.\n",
        "*   Split the sentences into train and test sets.\n",
        "*   Replace words with a low frequency by an unknown marker <unk>.\n",
        "\n",
        "\n",
        "2.   Develop N-gram based language models\n",
        "\n",
        "\n",
        "*   Compute the count of n-grams from a given data set.\n",
        "*   Estimate the conditional probability of a next word with k-smoothing.\n",
        "\n",
        "\n",
        "3. Use your own model to suggest an upcoming word given your sentence.\n",
        "\n"
      ],
      "metadata": {
        "id": "XI3oaszzBqB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.data.path.append('.')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRmB5nvlHCiY",
        "outputId": "3de6d281-1c26-485d-cd31-3ac28f1a2ac7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data\n"
      ],
      "metadata": {
        "id": "UeqnsFkVHhJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"en_US.twitter.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "display(data[0:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "em2ODgXaHjXd",
        "outputId": "06ebdc9d-5456-4bcd-d3ad-e32e2807940f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process the data\n",
        "\n",
        "reprocess this data with the following steps:\n",
        "\n",
        "1. Split data into sentences using \"\\n\" as the delimiter.\n",
        "2. Split each sentence into tokens.\n",
        "3. Assign sentences into train or test sets.\n",
        "4. Find tokens that appear at least N times in the training data.\n",
        "5. Replace tokens that appear less than N times by <unk>"
      ],
      "metadata": {
        "id": "8EfSAVNUIFYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into sentences\n",
        "\n",
        "    Split data by linebreak \"\\n\"\n",
        "    Args:\n",
        "        data: str\n",
        "    Returns:\n",
        "        A list of sentences\n"
      ],
      "metadata": {
        "id": "3wQIRcwtIoDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_to_sentences(data):\n",
        "\n",
        "    sentences = data.split('\\n')\n",
        "\n",
        "    # - Remove leading and trailing spaces from each sentence\n",
        "    # - Drop sentences if they are empty strings.\n",
        "    sentences = [sentence.strip() for sentence in sentences]\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "H79n1nryIR0I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test your code\n",
        "x = \"\"\"I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\"\"\"\n",
        "print(x)\n",
        "\n",
        "split_to_sentences(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3woV0lfiLWL5",
        "outputId": "5b60f487-33a6-49d2-902e-76226f73f7a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have a pen.\n",
            "I have an apple. \n",
            "Ah\n",
            "Apple pen.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize sentences (split a sentence into a list of words).\n",
        "\n",
        "* Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words.\n",
        "* Append each tokenized list of words into a list of tokenized sentences\n",
        "\n",
        "\n",
        "    Tokenize sentences into tokens (words)\n",
        "    \n",
        "    Args:\n",
        "        sentences: List of strings\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of tokens"
      ],
      "metadata": {
        "id": "K8EBvfbILUIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences(sentences):\n",
        "\n",
        "    tokenized_sentences = []\n",
        "\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "        # Convert to lowercase letters\n",
        "        sentence = sentence.lower()\n",
        "\n",
        "        # Convert into a list of words\n",
        "        tokenized = nltk.word_tokenize(sentence)\n",
        "\n",
        "        # append the list of words to the list of lists\n",
        "        tokenized_sentences.append(tokenized)\n",
        "\n",
        "    return tokenized_sentences\n"
      ],
      "metadata": {
        "id": "vx95qECkMU9G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
        "tokenize_sentences(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6exUuJ6Mv-f",
        "outputId": "3e7abc50-3984-4588-fbd9-634cb6723565"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['sky', 'is', 'blue', '.'],\n",
              " ['leaves', 'are', 'green', '.'],\n",
              " ['roses', 'are', 'red', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split into train and test sets"
      ],
      "metadata": {
        "id": "4mnQUyVscUNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenized_data(data):\n",
        "    sentences = split_to_sentences(data)\n",
        "    tokenized_sentences = tokenize_sentences(sentences)\n",
        "    return tokenized_sentences"
      ],
      "metadata": {
        "id": "KPJqjsF76KG9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = get_tokenized_data(data)\n",
        "random.seed(87)\n",
        "random.shuffle(tokenized_data)\n",
        "\n",
        "train_size = int(len(tokenized_data) * 0.8)\n",
        "train_data = tokenized_data[0:train_size]\n",
        "test_data = tokenized_data[train_size:]"
      ],
      "metadata": {
        "id": "Pf58ap-WcTlp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making the dictionary\n",
        "\n",
        "* focus on the words that appear at least N times in the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "4GVTRpZ7fYgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(tokenized_sentences):\n",
        "\n",
        "    word_counts = {}\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "\n",
        "        for token in sentence:\n",
        "            word_counts[token] = word_counts.get(token, 0) + 1\n",
        "\n",
        "    return word_counts"
      ],
      "metadata": {
        "id": "ZyBbouZngFmi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
        "                       ['leaves', 'are', 'green', '.'],\n",
        "                       ['roses', 'are', 'red', '.']]\n",
        "count_words(tokenized_sentences)"
      ],
      "metadata": {
        "id": "ldeC8OrqhFsf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d4dca7-daa7-4414-f1ca-f6b1be5bb8cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sky': 1,\n",
              " 'is': 1,\n",
              " 'blue': 1,\n",
              " '.': 3,\n",
              " 'leaves': 1,\n",
              " 'are': 2,\n",
              " 'green': 1,\n",
              " 'roses': 1,\n",
              " 'red': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling 'Out of Vocabulary' words\n",
        "\n",
        "To handle unknown words during prediction, use a special token to represent all unknown words 'unk'.\n",
        "\n",
        "* Modify the training data so that it has some 'unknown' words to train on.\n",
        "* Words to convert into \"unknown\" words are those that do not occur very frequently in the training set.\n",
        "* Create a list of the most frequent words in the training set, called the closed vocabulary .\n",
        "* Convert all the other words that are not part of the closed vocabulary to the token 'unk'."
      ],
      "metadata": {
        "id": "jvvZiLRThSuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
        "\n",
        "    closed_vocab = []\n",
        "    word_counts = count_words(tokenized_sentences)\n",
        "    # for each word and its count\n",
        "    for word, count in word_counts.items():\n",
        "        if count >= count_threshold:\n",
        "            # append the word to the list\n",
        "            closed_vocab.append(word)\n",
        "\n",
        "\n",
        "    return closed_vocab"
      ],
      "metadata": {
        "id": "BydJQGnkpBtp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words that appear count_threshold times or more are in the closed vocabulary.\n",
        "\n",
        "* All other words are regarded as unknown.\n",
        "* Replace words not in the closed vocabulary with the token $<unk>$."
      ],
      "metadata": {
        "id": "7Vz0RglQieqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token='<unk>'):\n",
        "\n",
        "    vocabulary = set(vocabulary)\n",
        "\n",
        "    # Initialize a list that will hold the sentences\n",
        "    # after less frequent words are replaced by the unknown token\n",
        "    replaced_tokenized_sentences = []\n",
        "\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "\n",
        "        # Initialize the list that will contain\n",
        "        # a single sentence with \"unknown_token\" replacements\n",
        "        replaced_sentence = []\n",
        "        # for each token in the sentence\n",
        "        for token in sentence:\n",
        "            replaced_sentence.append(token if token in vocabulary else unknown_token)\n",
        "\n",
        "        replaced_tokenized_sentences.append(replaced_sentence)\n",
        "\n",
        "    return replaced_tokenized_sentences"
      ],
      "metadata": {
        "id": "yTi0re__kf8D"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(train_data, test_data, count_threshold):\n",
        "\n",
        "\n",
        "    # Get the closed vocabulary using the train data\n",
        "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
        "\n",
        "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary)\n",
        "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary)\n",
        "\n",
        "    return train_data_replaced, test_data_replaced, vocabulary"
      ],
      "metadata": {
        "id": "YujQtbi4oyfN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the train and test data"
      ],
      "metadata": {
        "id": "vENyzq19pdo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minimum_freq = 2\n",
        "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data, minimum_freq)"
      ],
      "metadata": {
        "id": "FJlx8tvdpnf4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Developing the  n-gram based language\n",
        "\n",
        "Asumptions\n",
        "\n",
        "* Assume the probability of the next word depends only on the previous n-gram.\n",
        "* The previous n-gram is the series of the previous 'n' words\n",
        "\n",
        "\n",
        "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $$w_{t-1}, w_{t-2} \\cdots w_{t-n}$$ is:\n",
        "$P(w_t | w_{t-1}\\dots w_{t-n})$\n",
        "  \n",
        "\n",
        "- The probability can be estimated as a ratio, where\n",
        "- The numerator is the number of times word 't' appears after words t-1 through t-n appear in the training data.\n",
        "- The denominator is the number of times word t-1 through t-n appears in the training data.\n",
        "\n",
        "    $$ \\hat{P}(w_t | w_{t-1}\\\\dots w_{t-n}) = \\\\frac{C(w_{t-1}\\\\dots w_{t-n}, w_n)}{C(w_{t-1}\\\\dots w_{t-n})}  $$\n",
        "\n",
        "\n",
        "The function $C(\\cdots)$ denotes the number of occurence of the given sequence.\n",
        "$\\hat{P}$ means the estimation of $P$.\n",
        "\n",
        "  The equation tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator)."
      ],
      "metadata": {
        "id": "qt7PYb5TqfpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Count The N-Grams\n",
        "\n",
        " function that computes the counts of n-grams for an arbitrary number $n$\n",
        "\n",
        " When computing the counts for n-grams, prepare the sentence beforehand by prepending $n-1$ starting markers $<s>$ to indicate the beginning of the sentence."
      ],
      "metadata": {
        "id": "1YiD_k4Bu7Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_n_grams(data, n, start_token='<s>', end_token='<e>'):\n",
        "\n",
        "    n_grams = {}\n",
        "\n",
        "    for sentence in data:\n",
        "        sentence = [start_token] * n + sentence + [end_token]\n",
        "\n",
        "        sentence = tuple(sentence)\n",
        "\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            n_gram = sentence[i : i + n]\n",
        "            n_grams[n_gram] = n_grams.get(n_gram, 0) + 1\n",
        "\n",
        "    return n_grams"
      ],
      "metadata": {
        "id": "BPWM9-zXqs71"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "print(\"Uni-gram:\")\n",
        "print(count_n_grams(sentences, 1))\n",
        "print(\"Bi-gram:\")\n",
        "print(count_n_grams(sentences, 2))"
      ],
      "metadata": {
        "id": "oojp4QcuxVpr",
        "outputId": "389f8694-faff-462d-e195-d43f26dc5a9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uni-gram:\n",
            "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
            "Bi-gram:\n",
            "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimating probability\n",
        "\n",
        "we  estimate the probability of a word given the prior 'n' words using the n-gram counts.\n",
        "\n",
        "$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} $\n",
        "    \n",
        "This formula doesn't work when a count of an n-gram is zero..\n",
        "  \n",
        "  - Suppose we encounter an n-gram that did not occur in the training data.\n",
        "  - Then, the equation cannot be evaluated (it becomes zero divided by zero).\n",
        "  A way to handle zero counts is to add k-smoothing.\n",
        "  \n",
        "  - K-smoothing adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary.\n",
        "    \n",
        "  $ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|}  $\n",
        "\n",
        "  For n-grams that have a zero count, the equation  becomes $\\\\frac{1}{|V|}$\n",
        "  - This means that any n-gram with zero count has the same probability of $\\\\frac{1}{|V|}$"
      ],
      "metadata": {
        "id": "MsP0me_lMB7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probability(word, previous_n_gram,\n",
        "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "\n",
        "\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
        "\n",
        "    # apply k-smoothing\n",
        "    denominator = previous_n_gram_count + k * vocabulary_size\n",
        "\n",
        "    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n",
        "    n_plus1_gram = previous_n_gram + (word,)\n",
        "\n",
        "    # Set the count to the count in the dictionary,\n",
        "    # otherwise 0 if not in the dictionary\n",
        "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
        "\n",
        "    # Define the numerator  apply smoothing\n",
        "    numerator = n_plus1_gram_count + k\n",
        "\n",
        "    # Calculate the probability as the numerator divided by denominator\n",
        "    probability = numerator / denominator\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return probability"
      ],
      "metadata": {
        "id": "ISOPPvhNMJng"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
        "\n",
        "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86djPVhvZ5vT",
        "outputId": "bbce14d1-c83d-4923-e6f1-f40cc96ff004"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimate probabilities for all words\n",
        "\n",
        "The function defined below loops over all words in vocabulary to calculate probabilities for all possible words."
      ],
      "metadata": {
        "id": "cwLwcrB_akyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "    vocabulary = vocabulary + ['<e>', '<unk>']\n",
        "    vocabulary_size = len(vocabulary)\n",
        "\n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        probability = estimate_probability(word, previous_n_gram,\n",
        "                                           n_gram_counts, n_plus1_gram_counts,\n",
        "                                           vocabulary_size, k=k)\n",
        "        probabilities[word] = probability\n",
        "\n",
        "    return probabilities"
      ],
      "metadata": {
        "id": "PmqMKz1oarJu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLAmIOjigIV_",
        "outputId": "e1cfc2bf-2791-4f45-b94e-8394c357c400"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is': 0.09090909090909091,\n",
              " 'this': 0.09090909090909091,\n",
              " 'dog': 0.09090909090909091,\n",
              " 'like': 0.09090909090909091,\n",
              " 'a': 0.09090909090909091,\n",
              " 'i': 0.09090909090909091,\n",
              " 'cat': 0.2727272727272727,\n",
              " '<e>': 0.09090909090909091,\n",
              " '<unk>': 0.09090909090909091}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count and probability matrices\n",
        "\n"
      ],
      "metadata": {
        "id": "aYTlOlbmgRIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "\n",
        "    # obtain unique n-grams\n",
        "    n_grams = []\n",
        "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        n_grams.append(n_gram)\n",
        "    n_grams = list(set(n_grams))\n",
        "\n",
        "    # mapping from n-gram to row\n",
        "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
        "    # mapping from next word to column\n",
        "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
        "\n",
        "    nrow = len(n_grams)\n",
        "    ncol = len(vocabulary)\n",
        "    count_matrix = np.zeros((nrow, ncol))\n",
        "\n",
        "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        word = n_plus1_gram[-1]\n",
        "        if word not in vocabulary:\n",
        "            continue\n",
        "        i = row_index[n_gram]\n",
        "        j = col_index[word]\n",
        "        count_matrix[i, j] = count\n",
        "\n",
        "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
        "    return count_matrix"
      ],
      "metadata": {
        "id": "xZQGrIMWhU8T"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this function calculates the probabilities of each word given the previous n-gram, and stores this in matrix form"
      ],
      "metadata": {
        "id": "NRhjs_aZi_5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
        "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
        "    count_matrix += k\n",
        "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
        "    return prob_matrix"
      ],
      "metadata": {
        "id": "_j1Pf7Ski_ZX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Building the system auto-complete system\n"
      ],
      "metadata": {
        "id": "-0fQVCCksiK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
        "\n",
        "\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "\n",
        "    # Estimate the probabilities that each word in the vocabulary\n",
        "    probabilities = estimate_probabilities(previous_n_gram,\n",
        "                                           n_gram_counts, n_plus1_gram_counts,\n",
        "                                           vocabulary, k=k)\n",
        "\n",
        "    # Initialize suggested word to None\n",
        "    suggestion = None\n",
        "    # Initialize the highest word probability to 0\n",
        "    max_prob = 0\n",
        "\n",
        "    # For each word and its probability in the probabilities dictionary:\n",
        "    for word, prob in probabilities.items():\n",
        "        if start_with:\n",
        "            if not word.startswith(start_with):\n",
        "                continue\n",
        "\n",
        "        # Check if this word's probability\n",
        "        # is greater than the current maximum probability\n",
        "        if prob > max_prob:\n",
        "            suggestion = word\n",
        "            max_prob = prob\n",
        "\n",
        "    return suggestion, max_prob"
      ],
      "metadata": {
        "id": "lUDDPRgXon1P"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "previous_tokens = [\"i\", \"like\"]\n",
        "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
        "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "tmp_starts_with = 'c'\n",
        "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
        "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BP6DeTn2ftq",
        "outputId": "27e85600-4bc7-4bf1-aa23-e35842fba8be"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The previous words are 'i like',\n",
            "\tand the suggested word is `a` with a probability of 0.2727\n",
            "\n",
            "The previous words are 'i like', the suggestion must start with `c`\n",
            "\tand the suggested word is `cat` with a probability of 0.0909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### multiple suggestions"
      ],
      "metadata": {
        "id": "hFrfEqqw4i3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "loop over various n-gram models to get multiple suggestions."
      ],
      "metadata": {
        "id": "vBdsul0w4p4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "    model_counts = len(n_gram_counts_list)\n",
        "    suggestions = []\n",
        "    for i in range(model_counts-1):\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
        "\n",
        "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
        "                                    n_plus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        suggestions.append(suggestion)\n",
        "    return suggestions"
      ],
      "metadata": {
        "id": "-sFxlrZD4ic3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Tests"
      ],
      "metadata": {
        "id": "8p_IhaRv6pFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "\n",
        "    n_model_counts = count_n_grams(train_data_processed, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ],
      "metadata": {
        "id": "VvfpK9HH68ZY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previous_tokens = [\"this\", 'is']\n",
        "tmp_suggest = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "laIrMLH06pXP",
        "outputId": "432e5f4b-36fc-4a38-ec63-786167b62e66"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The previous words are ['this', 'is'], the suggestions are:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('the', 0.020172225129644606),\n",
              " ('the', 0.0022552401167418414),\n",
              " ('i', 6.745362563237774e-05),\n",
              " ('i', 6.745362563237774e-05)]"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}