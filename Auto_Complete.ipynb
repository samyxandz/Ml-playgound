{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP3OcO+I2YCrm9OCSRGTQTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyxandz/Ml-playgound/blob/main/Auto_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Models: Auto-Complete\n"
      ],
      "metadata": {
        "id": "a6Hay2_uBS7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Prototype uses N-grams, a simple but powerful method for language modeling."
      ],
      "metadata": {
        "id": "rnr3BRqbBXpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RoadMap\n",
        "\n",
        "<br>\n",
        "\n",
        "1.   Load and preprocess data\n",
        "Load and tokenize data.\n",
        "\n",
        "*   Load and tokenize data.\n",
        "*   Split the sentences into train and test sets.\n",
        "*   Replace words with a low frequency by an unknown marker <unk>.\n",
        "\n",
        "\n",
        "2.   Develop N-gram based language models\n",
        "\n",
        "\n",
        "*   Compute the count of n-grams from a given data set.\n",
        "*   Estimate the conditional probability of a next word with k-smoothing.\n",
        "\n",
        "3. Evaluate the N-gram models by computing the perplexity score.\n",
        "\n",
        "4. Use your own model to suggest an upcoming word given your sentence.\n",
        "\n"
      ],
      "metadata": {
        "id": "XI3oaszzBqB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.data.path.append('.')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRmB5nvlHCiY",
        "outputId": "6a0a99a2-c06f-410c-bd72-ca3e69bf57e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data\n"
      ],
      "metadata": {
        "id": "UeqnsFkVHhJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"en_US.twitter.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "display(data[0:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "em2ODgXaHjXd",
        "outputId": "407bff06-7d5c-4e3f-93a9-5baa3bdc6732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process the data\n",
        "\n",
        "reprocess this data with the following steps:\n",
        "\n",
        "1. Split data into sentences using \"\\n\" as the delimiter.\n",
        "2. Split each sentence into tokens.\n",
        "3. Assign sentences into train or test sets.\n",
        "4. Find tokens that appear at least N times in the training data.\n",
        "5. Replace tokens that appear less than N times by <unk>"
      ],
      "metadata": {
        "id": "8EfSAVNUIFYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into sentences\n",
        "\n",
        "    Split data by linebreak \"\\n\"\n",
        "    Args:\n",
        "        data: str\n",
        "    Returns:\n",
        "        A list of sentences\n"
      ],
      "metadata": {
        "id": "3wQIRcwtIoDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_to_sentences(data):\n",
        "\n",
        "    sentences = data.split('\\n')\n",
        "\n",
        "    # - Remove leading and trailing spaces from each sentence\n",
        "    # - Drop sentences if they are empty strings.\n",
        "    sentences = [sentence.strip() for sentence in sentences]\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "H79n1nryIR0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test your code\n",
        "x = \"\"\"I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\"\"\"\n",
        "print(x)\n",
        "\n",
        "split_to_sentences(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3woV0lfiLWL5",
        "outputId": "dbfd08c4-5bf5-4d08-94aa-81684b905025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have a pen.\n",
            "I have an apple. \n",
            "Ah\n",
            "Apple pen.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize sentences (split a sentence into a list of words).\n",
        "\n",
        "* Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words.\n",
        "* Append each tokenized list of words into a list of tokenized sentences\n",
        "\n",
        "\n",
        "    Tokenize sentences into tokens (words)\n",
        "    \n",
        "    Args:\n",
        "        sentences: List of strings\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of tokens"
      ],
      "metadata": {
        "id": "K8EBvfbILUIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences(sentences):\n",
        "\n",
        "    tokenized_sentences = []\n",
        "\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "        # Convert to lowercase letters\n",
        "        sentence = sentence.lower()\n",
        "\n",
        "        # Convert into a list of words\n",
        "        tokenized = nltk.word_tokenize(sentence)\n",
        "\n",
        "        # append the list of words to the list of lists\n",
        "        tokenized_sentences.append(tokenized)\n",
        "\n",
        "    return tokenized_sentences\n"
      ],
      "metadata": {
        "id": "vx95qECkMU9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
        "tokenize_sentences(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6exUuJ6Mv-f",
        "outputId": "379dfc20-bb6c-4ef1-9f67-b3199f0f6542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['sky', 'is', 'blue', '.'],\n",
              " ['leaves', 'are', 'green', '.'],\n",
              " ['roses', 'are', 'red', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split into train and test sets"
      ],
      "metadata": {
        "id": "4mnQUyVscUNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = get_tokenized_data(data)\n",
        "random.seed(87)\n",
        "random.shuffle(tokenized_data)\n",
        "\n",
        "train_size = int(len(tokenized_data) * 0.8)\n",
        "train_data = tokenized_data[0:train_size]\n",
        "test_data = tokenized_data[train_size:]"
      ],
      "metadata": {
        "id": "Pf58ap-WcTlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making the dictionary\n",
        "\n",
        "* focus on the words that appear at least N times in the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "4GVTRpZ7fYgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(tokenized_sentences):\n",
        "\n",
        "    word_counts = {}\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "\n",
        "        for token in sentence:\n",
        "            word_counts[token] = word_counts.get(token, 0) + 1\n",
        "\n",
        "    return word_counts"
      ],
      "metadata": {
        "id": "ZyBbouZngFmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
        "                       ['leaves', 'are', 'green', '.'],\n",
        "                       ['roses', 'are', 'red', '.']]\n",
        "count_words(tokenized_sentences)"
      ],
      "metadata": {
        "id": "ldeC8OrqhFsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling 'Out of Vocabulary' words\n",
        "\n",
        "To handle unknown words during prediction, use a special token to represent all unknown words 'unk'.\n",
        "\n",
        "* Modify the training data so that it has some 'unknown' words to train on.\n",
        "* Words to convert into \"unknown\" words are those that do not occur very frequently in the training set.\n",
        "* Create a list of the most frequent words in the training set, called the closed vocabulary .\n",
        "* Convert all the other words that are not part of the closed vocabulary to the token 'unk'."
      ],
      "metadata": {
        "id": "jvvZiLRThSuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
        "\n",
        "    closed_vocab = []\n",
        "    word_counts = count_words(tokenized_sentences)\n",
        "    # for each word and its count\n",
        "    for word, count in word_counts.items():\n",
        "\n",
        "        # check that the word's count\n",
        "        # is at least as great as the minimum count\n",
        "        if count >= count_threshold:\n",
        "            # append the word to the list\n",
        "            closed_vocab.append(word)\n",
        "\n",
        "\n",
        "    return closed_vocab"
      ],
      "metadata": {
        "id": "BydJQGnkpBtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words that appear count_threshold times or more are in the closed vocabulary.\n",
        "\n",
        "* All other words are regarded as unknown.\n",
        "* Replace words not in the closed vocabulary with the token $<unk>$."
      ],
      "metadata": {
        "id": "7Vz0RglQieqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token='<unk>'):\n",
        "\n",
        "    vocabulary = set(vocabulary)\n",
        "\n",
        "    # Initialize a list that will hold the sentences\n",
        "    # after less frequent words are replaced by the unknown token\n",
        "    replaced_tokenized_sentences = []\n",
        "\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "\n",
        "        # Initialize the list that will contain\n",
        "        # a single sentence with \"unknown_token\" replacements\n",
        "        replaced_sentence = []\n",
        "        # for each token in the sentence\n",
        "        for token in sentence:\n",
        "            replaced_sentence.append(token if token in vocabulary else unknown_token)\n",
        "\n",
        "        replaced_tokenized_sentences.append(replaced_sentence)\n",
        "\n",
        "    return replaced_tokenized_sentences"
      ],
      "metadata": {
        "id": "yTi0re__kf8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(train_data, test_data, count_threshold):\n",
        "\n",
        "\n",
        "    # Get the closed vocabulary using the train data\n",
        "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
        "\n",
        "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary)\n",
        "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary)\n",
        "\n",
        "    return train_data_replaced, test_data_replaced, vocabulary"
      ],
      "metadata": {
        "id": "YujQtbi4oyfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the train and test data"
      ],
      "metadata": {
        "id": "vENyzq19pdo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minimum_freq = 2\n",
        "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data, minimum_freq)"
      ],
      "metadata": {
        "id": "FJlx8tvdpnf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Developing the  n-gram based language\n",
        "\n",
        "Asumptions\n",
        "\n",
        "* Assume the probability of the next word depends only on the previous n-gram.\n",
        "* The previous n-gram is the series of the previous 'n' words\n",
        "\n",
        "\n",
        "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $$w_{t-1}, w_{t-2} \\cdots w_{t-n}$$ is:\n",
        "$P(w_t | w_{t-1}\\dots w_{t-n})$\n",
        "  \n",
        "\n",
        "- The probability can be estimated as a ratio, where\n",
        "- The numerator is the number of times word 't' appears after words t-1 through t-n appear in the training data.\n",
        "- The denominator is the number of times word t-1 through t-n appears in the training data.\n",
        "\n",
        "    $$ \\hat{P}(w_t | w_{t-1}\\\\dots w_{t-n}) = \\\\frac{C(w_{t-1}\\\\dots w_{t-n}, w_n)}{C(w_{t-1}\\\\dots w_{t-n})}  $$\n",
        "\n",
        "\n",
        "The function $C(\\cdots)$ denotes the number of occurence of the given sequence.\n",
        "$\\hat{P}$ means the estimation of $P$.\n",
        "\n",
        "  The equation tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator)."
      ],
      "metadata": {
        "id": "qt7PYb5TqfpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Count The N-Grams\n",
        "\n",
        " function that computes the counts of n-grams for an arbitrary number $n$\n",
        "\n",
        " When computing the counts for n-grams, prepare the sentence beforehand by prepending $n-1$ starting markers $<s>$ to indicate the beginning of the sentence."
      ],
      "metadata": {
        "id": "1YiD_k4Bu7Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_n_grams(data, n, start_token='<s>', end_token='<e>'):\n",
        "\n",
        "    n_grams = {}\n",
        "\n",
        "    for sentence in data:\n",
        "        sentence = [start_token] * n + sentence + [end_token]\n",
        "\n",
        "        sentence = tuple(sentence)\n",
        "\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            n_gram = sentence[i : i + n]\n",
        "            n_grams[n_gram] = n_grams.get(n_gram, 0) + 1\n",
        "\n",
        "    return n_grams"
      ],
      "metadata": {
        "id": "BPWM9-zXqs71"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "print(\"Uni-gram:\")\n",
        "print(count_n_grams(sentences, 1))\n",
        "print(\"Bi-gram:\")\n",
        "print(count_n_grams(sentences, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oojp4QcuxVpr",
        "outputId": "9d6b3263-0535-4967-8e99-20454f9a0f68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uni-gram:\n",
            "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
            "Bi-gram:\n",
            "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
          ]
        }
      ]
    }
  ]
}