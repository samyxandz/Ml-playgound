{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH6jn6BbJt2IETvg8sTjGp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyxandz/Ml-playgound/blob/main/Corpus%20preprocessing/Corpus_preprocessing_N_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing steps for the language models include:\n",
        "\n",
        "- lowercasing the text\n",
        "- remove special characters\n",
        "- split text to list of sentences\n",
        "- split sentence into list words"
      ],
      "metadata": {
        "id": "0mPlK_XMvBZT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kkymrwe0fhIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd34f92-436b-45ba-94da-4a2d93c40044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk               # NLP toolkit\n",
        "import re                 # Library for Regular expression operations\n",
        "\n",
        "nltk.download('punkt')    # Download the Punkt sentence tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lowercase"
      ],
      "metadata": {
        "id": "ZLuREgDbfTgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
        "corpus = corpus.lower()\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3KfWOGifTpY",
        "outputId": "c12fd840-69c3-45f2-f0e4-3d20867e57d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Remove special charactes"
      ],
      "metadata": {
        "id": "AMQBZ9xyfXoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = re.sub(r'[^a-zA-Z0-9.?! ]+', '', corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-pvTLrAfXzZ",
        "outputId": "9ae5fa65-76cd-474a-9231-487c2393e2c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning makes me happy. i am happy because i am learning! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text splitting\n",
        "\n",
        "The sentences in the corpus are separated by a special delimiter \\n so we split the corpus into an array of sentences using this delimiter.\n",
        " One way to do that is by using the str.split method.\n",
        "\n"
      ],
      "metadata": {
        "id": "r90ees4hheWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data=\"Sat May  9 07:33:35 CEST 2020\"\n",
        "data_parts = input_data.split(' ')"
      ],
      "metadata": {
        "id": "qkG2Ca1eheh4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence tokenizing"
      ],
      "metadata": {
        "id": "SaNWfWp7iHr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'i am happy because i am learning.'\n",
        "tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "print(f'{sentence} -> {tokenized_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCLnjKDsiH0_",
        "outputId": "18e877fb-8145-4641-f483-e3dba091b73b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am happy because i am learning. -> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence to n-gram\n"
      ],
      "metadata": {
        "id": "HpmwD3skpdkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_trigram(tokenized_sentence):\n",
        "\n",
        "    # note that the last position of i is 3rd to the end\n",
        "    for i in range(len(tokenized_sentence) - 3 + 1):\n",
        "        # the sliding window starts at position i and contains 3 words\n",
        "        trigram = tokenized_sentence[i : i + 3]\n",
        "        print(trigram)\n",
        "\n",
        "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "\n",
        "print(f'List all trigrams of sentence: {tokenized_sentence}\\n')\n",
        "sentence_to_trigram(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fxRX2O9iovV",
        "outputId": "e757a946-6afd-49a7-eea0-8e5cb714892f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
            "\n",
            "['i', 'am', 'happy']\n",
            "['am', 'happy', 'because']\n",
            "['happy', 'because', 'i']\n",
            "['because', 'i', 'am']\n",
            "['i', 'am', 'learning']\n",
            "['am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start and end of sentence word\n",
        "\n",
        "we must add some special characters at the beginning and the end of each sentence:\n",
        "\n",
        "- $<s>$ at beginning\n",
        "- $</s> $at the end\n"
      ],
      "metadata": {
        "id": "WYS_kV1KpypI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "tokenized_sentence = ['<s>'] * (n - 1) + tokenized_sentence + ['</s>']\n",
        "print(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRT8k4HGp0MS",
        "outputId": "ddbe86dc-fc74-4bb4-e893-8a2a0c9ae2f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '</s>']\n"
          ]
        }
      ]
    }
  ]
}